## Announcements
### Exam 1 Grades
Very good!
### Lab 4
Try to complete it by the wellness days (already done :))  
### No Async lectures or office hours over wellness days

## Memory Concepts - Caches
### Fully Associative Cache Example:
2 tag bits identify the block, 2 tag bits identify the
offset  
When the CPU goes to cache for `0001`, it would look for the
tag `00` in cache and find a cache hit!  
Then it would look for the offset `01` in that cache block  

One of the issues with fully associative is that if there is
a miss in cache, you have to replace a block in cache  
We should remove blocks from the cache that aren't being hit  

#### LRU (Least-recently used)
Replaces the item that has gone unaccessed the longest  
Favors the most recently accessed data  

#### FIFO/LRR (first-in, first-out/least-recently replaced)
Replaces the item that has been in cache the longest  
Favors recently loaded items over older STALE items  

#### Random
Replace some item at RANDOM  

### Direct Mapping Read Cache Example:
Memory addresses get a Line, Tag, and Offset  
For `0101`, we have one tag bit (`0`), one line bit (`1`),
and two offset bits (`01`)  
There is no replacement, we just put the address on the line
it is addressed for  
NOW **tag and line** bits give the location of the block  

**Requested data can only be in one location**  
Cache location is directly determined by the actual memory
address  
**Fairly rigid storage strategy**  
A piece of data can only go in one spot  

#### LOOK AT THE PICTURE OF CACHE WALKTHROUGH

### Why Blocks?
**They improve locality!**  
Principle of Locality: Programs tend to use data and
instructions with addresses near or equal to those they have
used recently  
*Temporal Locality:*  
Recently referenced items are likely to be referenced again
in the near future - SAME ADDRESS LOCATION  
*Spatial Locality:*  
Items with nearby addresses tend to be referenced close
together in time  

### Handling WRITES
**Most (80%+) of memory accesses are reads, but writes are
essential**  

Two different policies  
**Write-through:**  
CPU writes are cached, but also written to main memory
(stalling the CPU until write is completed), memory always
holds the latest values  
**Write-back:**  
CPU writes are cached, but not immediately written to main
memory, main memory contents can become "stale"  
Only when a value has to be evicted from cache, and only if
it has been modified, it is written to main memory  

**PROS and CONS**  
*Write-through* typically causes fewer consistency problems  
*Write-back* typically has higher performance  

### CACHE SUMMARY
**Partition memory into blocks**  
Improves locality  
**Very simple cache read examples**  
Fully associative (a = t + w)  
Direct mapping (a = t + l + w)  
**In simple examples, 1 word = 1 byte**  
In reality, size of word is # of address bits  
So on a 32-bit OS, 1 word = 32 bits = 4 bytes  
On a 64-bit OS, 1 word = 64 bits = 8 bytes  
**NUMBER OF BLOCKS = a / word size**  

## Virtual Memory Design
Provides communication between CPU and physical memory  
Virtual addresses made up of Page number and Offset  

Page number correlates to a Frame number in the page table  
Then the physical address is the combination of the Frame
number and Offset  

Virtual memory can be larger than the actual bit system  
More on Virtual memory next class
